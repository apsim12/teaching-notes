{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Discussion\n",
    "\n",
    "## Infographic\n",
    "\n",
    "* [The Pudding](https://pudding.cool/)\n",
    "* [The State of The Pudding, 2018](https://medium.com/@matthew_daniels/the-state-of-the-pudding-2018-9661ab4d299c)\n",
    "\n",
    "## Links\n",
    "\n",
    "* [MDN HTML Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "* [CSS Diner](https://flukeout.github.io/) -- an interactive CSS Selector tutorial\n",
    "* [XPath Diner](http://www.topswagcode.com/xpath/) -- an interactive XPath tutorial\n",
    "* [Wiki's XPath Page][xpath]\n",
    "* [Python's Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html#regex-howto)\n",
    "\n",
    "__Next week only:__ my office hours will be at a different time (TBA soon on Slack).\n",
    "\n",
    "[xpath]: https://en.wikipedia.org/wiki/XPath#Syntax_and_semantics_(XPath_1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Basic web scraping workflow:\n",
    "\n",
    "1. Download pages\n",
    "2. _Parse_ pages to extract text\n",
    "3. Clean up extracted text\n",
    "4. Store cleaned results\n",
    "5. Analyze results\n",
    "\n",
    "This workflow is the same regardless of the packages and language you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Python\n",
    "\n",
    "We've already seen how to use [requests][] (with [requests_cache][]) to download pages (use a `GET` request).\n",
    "\n",
    "The packages [lxml][] and [Beautiful Soup][bs4] can parse web pages. Choose one for the entire scrape, since they are not compatible with each other.\n",
    "\n",
    "You can clean up extracted text with [string methods][str], [re][], and [pandas][]. Sometimes you'll also need to use natural language processing packages.\n",
    "\n",
    "The [scrapy][] and [pattern][] packages try to automate steps 1-4. They can save you time _after_ you understand the basics of scraping.\n",
    "\n",
    "[requests]: http://docs.python-requests.org/en/master/\n",
    "[requests_cache]: https://requests-cache.readthedocs.io/en/latest/\n",
    "\n",
    "[lxml]: http://lxml.de/lxmlhtml.html\n",
    "[bs4]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "[str]: https://docs.python.org/3/library/stdtypes.html#string-methods\n",
    "[re]: https://docs.python.org/3/library/re.html\n",
    "[pandas]: http://pandas.pydata.org/pandas-docs/stable/\n",
    "\n",
    "[scrapy]: https://scrapy.org/\n",
    "[pattern]: https://www.clips.uantwerpen.be/pages/pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes a web page?\n",
    "\n",
    "Web pages are written in hypertext markup language (HTML).\n",
    "\n",
    "In HTML, text is surrounded by _tags_ to mark formatting. Tags are written inside of angle brackets `< >`. For example,  to mark some text as bold:\n",
    "```html\n",
    "This is a <b>cat</b>!\n",
    "```\n",
    "\n",
    "Tags usually come in pairs; the second tag (with `/`) is called a _closing tag_ and marks the end of the formatting.\n",
    "\n",
    "Tags often have additional information as _attributes_. For example, links have an `href` attribute to specify the URL to link to:\n",
    "```html\n",
    "This <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">page</a> is famous.\n",
    "```\n",
    "\n",
    "A longer example of a web page is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>This is the Title!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "<p>This is a paragraph!</p>\n",
    "<p id=\"best-paragraph\">This is another paragraph!</p>\n",
    "<p>Visit <a href=\"http://www.google.com\">Google</a>.</p>\n",
    "<span>This is a span. ❤️ </span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firefox and Chrome come with (web) developer tools. You can use these to inspect a web page.\n",
    "\n",
    "Open the tools with `ctrl`-`shift`-`i` (or `cmd`-`shift`-`i` on OS X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Tags\n",
    "\n",
    "Tags are nested, so an HTML document is like a tree:\n",
    "```\n",
    "html\n",
    "├── head\n",
    "│   └── title\n",
    "└── body\n",
    "    ├── p\n",
    "    ├── p\n",
    "    ├── p\n",
    "    │   └── a\n",
    "    └── span\n",
    "```\n",
    "This is similar to the file system on your computer. The key difference is that tags at the same level can have the same name.\n",
    "\n",
    "XPath lets us write a path to a tag the same way we would to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tags can have the same name, we get a list rather than a single tag.\n",
    "\n",
    "Absolute paths are not convenient (and not robust) for scraping.\n",
    "\n",
    "In XPath, `//` means \"anywhere below\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `[ ]` to put a condition on a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath is not Python-specific!\n",
    "\n",
    "CSS Selectors are an alternative to XPath. They're less powerful but more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Scraping Hacker News\n",
    "\n",
    "Let's scrape <https://news.ycombinator.com/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "Python's built-in string methods are very powerful and very fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of them can also be used with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we need to extract more complicated patterns.\n",
    "\n",
    "In that case, we can use _regular expressions_. Regular expressions (or regex) is a language for describing patterns. It is not Python-specific.\n",
    "\n",
    "You can use regular expressions with the built-in `re` module or any of the pandas string methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Syntax\n",
    "\n",
    "In regular expressions, letters, numbers, and spaces are matched literally.\n",
    "\n",
    "Other characters have special meanings:\n",
    "\n",
    "Character | Description\n",
    "--------- | -----------\n",
    "`.`       | any 1 character\n",
    "`[ ]`     | any 1 character listed\n",
    "`?`       | repeat previous 0 or 1 times\n",
    "`*`       | repeat previous 0 or more times\n",
    "`+`       | repeat previous 1 or more times\n",
    "`^`       | start of string\n",
    "`$`       | end of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can match a special character literally by putting a backslash `\\` in front of it.\n",
    "\n",
    "The backslash `\\` also has a special meaning to Python, so in an ordinary string we'd have to use two backslashes:\n",
    "```\n",
    "\"\\\\.\"\n",
    "```\n",
    "This is so annoying that Python also supports _raw strings_, where backslash has no special meaning (to Python). These have an `r` before the quote character.\n",
    "\n",
    "Use raw strings for your regular expressions. Then you can just write:\n",
    "```\n",
    "r\"\\.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week's discussion will have more examples of regular expressions, and also some examples of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Wisely\n",
    "\n",
    "Before you scrape, is there an easier way to get the data? A direct data download? A web API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ How do I scrape the \"next page\" (for example, in a news listing)?\n",
    "\n",
    "__A:__ The link to the next page is on the first page, so have your scraper get that link. Then use a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ I tried scraping a web page but what I see in my browser doesn't match what I see in the request response. What's going on? ([example](https://ucannualwage.ucop.edu/wage/))\n",
    "\n",
    "__A:__ Several things could cause this.\n",
    "\n",
    "Most likely is that web page has JavaScript code that displays the data. Your web browser runs the code, but a simple HTTP request does not. If the JavaScript gets the data from a web API, try to reverse-engineer the API (you can figure this out with the \"Network\" tab in the browser dev tools).\n",
    "\n",
    "If the JavaScript doesn't use a web API, you can use tools like [selenium][] or [pyppeteer][] to control the browser from Python. The disadvantage is that this can be very slow if you need to scrape a lot of pages.\n",
    "\n",
    "Some web pages are deliberately hard to scrape because the owners don't want you to have their data.\n",
    "\n",
    "[selenium]: http://selenium-python.readthedocs.io/\n",
    "[pyppeteer]: https://github.com/miyakogi/pyppeteer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ How do I scrape a page where I have to log in?\n",
    "\n",
    "__A:__ Use selenium or pyppeteer, as mentioned above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
