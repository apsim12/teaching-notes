{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Discussion\n",
    "\n",
    "## Infographic\n",
    "\n",
    "* [History of Infographics](http://infowetrust.com/scroll/)\n",
    "\n",
    "## Links\n",
    "\n",
    "* [Python's Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html#regex-howto)\n",
    "* [RegExr](https://regexr.com/) -- test regex online\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "\n",
    "A _regular expression_, or regex, is a language for expressing patterns in strings.\n",
    "\n",
    "Regular expressions are not Python-specific! They are also supported in R, Java, Perl, etc...\n",
    "\n",
    "In Python, regex is supported by the built-in [`re` module][re] as well as the [`regex` package][regex] and some of the Pandas [`.str` methods][pandas-str].\n",
    "\n",
    "Regular expressions are __slow and brittle__. Use them as a last resort! Try to solve problems with [string methods][str] or [an appropriate parser][lxml] instead. There's even a [famous SO post][so-regex-html] about this.\n",
    "\n",
    "[re]: https://docs.python.org/3/library/re.html\n",
    "[regex]: https://pypi.python.org/pypi/regex/\n",
    "[pandas-str]: https://pandas.pydata.org/pandas-docs/stable/text.html\n",
    "[str]: https://docs.python.org/3/library/stdtypes.html#string-methods\n",
    "[lxml]: http://lxml.de/\n",
    "[so-regex-html]: https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metacharacters\n",
    "\n",
    "A regular expression can describe a complicated pattern in just a few characters because some non-alphabet characters have special meanings.\n",
    "\n",
    "These special characters are called metacharacters.\n",
    "\n",
    "Metacharacter | Meaning\n",
    "------------- | -------\n",
    "`.`           | any 1 character (a _wildcard_)\n",
    "`[ ]`         | any 1 character listed (a _set_)\n",
    "`[^ ]`        | any 1 character except those listed (a _set complement_)\n",
    "`^`           | start of string\n",
    "`$`           | end of string\n",
    "\n",
    "The `re.findall()` function returns a list of each part of the string that matched the pattern. Experimenting with `re.findall()` is a good way to learn regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'y', 'z']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# re.findall(PATTERN, STRING)\n",
    "re.findall(\".\", \"xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Square brackets `[ ]` mark a set. A set matches any 1 of the symbols inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'a', 'b']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[abc]\", \"caxyb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting `^` at the beginning of a set matches the _set complement_ (any character except the ones in the set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '4']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[^12]\", \"31214\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets can contain ranges of letters or numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', 'h', 'i']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[h-z0-9]\", \"0abcdefghi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include a literal dash `-` in a set, don't put it between letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z]\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z-]\", \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `^` and `$` to mark the start and end of a the string, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^a\", \"aba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escape Sequences\n",
    "\n",
    "The backslash `\\` has a special meaning in Python strings: it marks the beginning of an escape sequence.\n",
    "\n",
    "Escape sequences are necessary to write characters that don't appear on the keyboard. For instance, a newline is `\\n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Goodbye\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello Goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Goodbye\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because backslash marks the beginning of an escape sequence, if we want to write a literal backslash, we have to write `\\\\`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inconvenient since regex uses backslash to make a metacharacter into a literal character.\n",
    "\n",
    "So if we wanted to match a literal `.`, we'd have to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"\\\\.\"\n",
    "print(pattern)\n",
    "re.findall(pattern, \"x.y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even worse, to match a literal backslash, we'd have to write `\"\\\\\\\\\"`!\n",
    "\n",
    "Python provides _raw strings_ to fix this problem. In a raw string, backslash has no special meaning for Python (but it still has a special meaning for regex.\n",
    "\n",
    "To make a raw string, put the letter `r` before the quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"\\.\"\n",
    "print(pattern)\n",
    "re.findall(pattern, \"x.y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifiers\n",
    "\n",
    "You can describe repeated characters with quantifiers:\n",
    "\n",
    "Metacharacter | Meaning\n",
    "------------- | -------\n",
    "`*`           | repeat previous character 0 or more times\n",
    "`+`           | repeat previous character 1 or more times\n",
    "`?`           | repeat previous character 0 or 1 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'a']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"a\", \"aaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"a+\", \"aaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xz']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy?z\", \"xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyz']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy?z\", \"xyz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups\n",
    "\n",
    "You can make a group with parentheses `( )`.\n",
    "\n",
    "Groups can be repeated just like single characters.\n",
    "\n",
    "The `re.findall()` function has special handling for groups, so instead we use `re.search()` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 11), match='xabcabcabcy'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"x(abc)+y\", \"xabcabcabcy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "\n",
    "Basic NLP workflow:\n",
    "\n",
    "1. __Tokenize__ -- split text into words\n",
    "2. __Remove Noise__ (optional) -- remove stop words, convert words to lemmas, correct spelling, ...\n",
    "3. __Vectorize__ -- compute term frequencies, tf-idfs, or some other statistic\n",
    "4. __Analyze__\n",
    "\n",
    "This workflow is the same regardless of what language you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Python\n",
    "\n",
    "Python has lots of packages for natural language processing:\n",
    "\n",
    "* [TextBlob][] is the most Pythonic NLP package. Good for learning and prototyping.\n",
    "* [NLTK][] is the most comprehensive NLP package. Good for _learning_ NLP, but runs slowly.\n",
    "* [spaCy][] is the fastest NLP package.\n",
    "* [Stanford's Core NLP][CoreNLP] is the cutting edge of NLP research. It's written in Java, but several Python packages provide an interface. The [pynlp][] and [stanford-corenlp][] packages look promising.\n",
    "* [SyntaxNet][] is Google's NLP package. Good if you're already using TensorFlow.\n",
    "* [Pattern][] combines web scraping and NLP.\n",
    "* [gensim][] is good for topic modelling (a specific NLP method).\n",
    "\n",
    "Nick recommends __TextBlob__ with __NLTK__ for beginners, and __spaCy__ or __Core NLP__ for serious projects.\n",
    "\n",
    "[TextBlob]: http://textblob.readthedocs.io/en/dev/\n",
    "[NLTK]: https://www.nltk.org/\n",
    "[spaCy]: https://spacy.io/\n",
    "[CoreNLP]: https://stanfordnlp.github.io/CoreNLP/\n",
    "[pynlp]: https://github.com/sina-al/pynlp\n",
    "[stanford-corenlp]: https://github.com/Lynten/stanford-corenlp\n",
    "[SyntaxNet]: https://github.com/tensorflow/models/tree/master/research/syntaxnet\n",
    "[Pattern]: https://www.clips.uantwerpen.be/pattern\n",
    "[gensim]: https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nick/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nick/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /home/nick/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nick/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nick/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up NLTK packages used by TextBlob.\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"The quick brown foxes jumped lightly over the lazy dog.\"\n",
    "\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['The', 'quick', 'brown', 'foxes', 'jumped', 'lightly', 'over', 'the', 'lazy', 'dog'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Noise\n",
    "\n",
    "Sometimes it's useful to remove words that appear frequently, like \"the\". These are called _stopwords_.\n",
    "\n",
    "Removing stopwords is something you need to think about carefully. Does it make sense for the problem you're trying to solve?\n",
    "\n",
    "Also, the NLTK stopwords are not necessarily the right stopwords for your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"quick brown foxes jumped lightly lazy dog\")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "new_text = \" \".join(w for w in blob.words if w.lower() not in stopwords)\n",
    "blob = TextBlob(new_text)\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to removing stopwords, you might want to _lemmatize_ the words.\n",
    "\n",
    "Lemmatization converts words to a single inflection. After lemmatizing, plurals and verb tenses are eliminated.\n",
    "\n",
    "A related procedure, _stemming_, also converts words to a single inflection, but uses programmatic rules rather than a dictionary. Stemming is simpler but less accurate.\n",
    "\n",
    "As an example, if we lemmatize all of the words, \"foxes\" becomes \"fox\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['quick', 'brown', 'fox', 'jumped', 'lightly', 'lazy', 'dog'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why wasn't \"jumped\" converted to \"jump\"?\n",
    "\n",
    "By default, NLTK's lemmatizer only looks for nouns. We need to tell it which words are verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumped'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words[3].lemmatize(\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, TextBlob can detect parts of speech (POS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('foxes', 'NNS'),\n",
       " ('jumped', 'VBD'),\n",
       " ('lightly', 'RB'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are [Brown POS tags][brown], but the lemmatizer uses WordNet POS tags. Use this function to convert:\n",
    "\n",
    "[brown]: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the tags and use them to lemmatize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'n', 'n', 'v', 'r', 'a', 'n']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = [wordnet_pos(x[1]) for x in blob.pos_tags]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"quick brown fox jump lightly lazy dog\")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \" \".join(x.lemmatize(t) for x, t in zip(blob.words, tags))\n",
    "blob = TextBlob(new_text)\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencies\n",
    "\n",
    "How can we quantify a text document so we can do statistics?\n",
    "\n",
    "In a _bag of words_ model, we assume that the order of the words in a document doesn't matter.\n",
    "\n",
    "Then we can quantify the document by counting how many times each word appears. This is called a _word vector_.\n",
    "\n",
    "Surprisingly, the bag of words model works well in most cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'brown': 1,\n",
       "             'dog': 1,\n",
       "             'fox': 1,\n",
       "             'jumped': 1,\n",
       "             'lazy': 1,\n",
       "             'over': 1,\n",
       "             'quick': 1,\n",
       "             'the': 2})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how similar two documents are by computing the dot product of their word vectors.\n",
    "\n",
    "The more words they have in common, the larger the dot product will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"the quick brown fox\", \"the fast brown fox\"]\n",
    "\n",
    "def term_frequencies(doc):\n",
    "    return {w: x / doc.words for w, x in doc.word_counts}\n",
    "for doc in docs:\n",
    "    doc = TextBlob(doc)\n",
    "    {w: tf(doc) for w in doc.words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2\n",
       "a      0.0  1.0  0.0\n",
       "and    0.0  1.0  0.0\n",
       "are    0.0  1.0  0.0\n",
       "brown  1.0  1.0  1.0\n",
       "dog    1.0  0.0  0.0"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "docs = [\n",
    "    \"The quick brown foxes jumped lightly over the lazy dog.\",\n",
    "    \"Brown ducks lay eggs once a year and are omnivorous.\",\n",
    "    \"The fleet brown foxes jumped over the dogs.\"\n",
    "]\n",
    "\n",
    "# Get word counts.\n",
    "docs_df = pd.DataFrame(TextBlob(d).word_counts for d in docs).T\n",
    "docs_df = docs_df.fillna(0)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brown</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1      2\n",
       "a      0.0  0.1  0.000\n",
       "and    0.0  0.1  0.000\n",
       "are    0.0  0.1  0.000\n",
       "brown  0.1  0.1  0.125\n",
       "dog    0.1  0.0  0.000"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute frequencies.\n",
    "tf = docs_df / docs_df.sum()\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010000000000000002"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tf[0] * tf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tf[0] * tf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with term frequencies is that some terms have high frequencies simply because they appear frequently in the language. These words can cause a high similarity score for documents that are otherwise different.\n",
    "\n",
    "_Term frequency-inverse document frequency_ (tf-idf) statistics solve this problem. There are several different tf-idf statistics.\n",
    "\n",
    "The _smoothed term frequency-inverse document frequency_ (smoothed tf-idf), for a term $t$ and document $d$, is given by\n",
    "$$\n",
    "\\operatorname{tf-idf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's easiest to use a package to compute tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.04166004, 0.59997135],\n",
       "       [0.04166004, 1.        , 0.04772968],\n",
       "       [0.59997135, 0.04772968, 1.        ]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer().fit_transform(docs)\n",
    "\n",
    "# Use .A to display a sparse matrix.\n",
    "(tf_idf * tf_idf.T).A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
